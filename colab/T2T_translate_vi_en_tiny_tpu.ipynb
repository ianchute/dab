{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " T2T translate vi<->en tiny tpu",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vietai/dab/blob/master/colab/T2T_translate_vi%3C_%3Een_tiny_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odi2vIMHC3Rm",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this colab, we will train a translation model from English to/from Vietnamese using the Transformer architecture, making use of the Tensor2Tensor library. You will be shown how to use GPU/TPUv2 and how to connect to your Google Drive/Cloud Storage, all for free! To perform back-translation, you will need to run the entire colab the second time with the reverse direction of translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcfUjn9XePyl",
        "colab_type": "text"
      },
      "source": [
        "**MIT License**\n",
        "\n",
        "Copyright (c) [2019] [Trieu H. Trinh](https://thtrieu.github.io/)\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdTrQ4WIRIE2",
        "colab_type": "text"
      },
      "source": [
        "# Install dependencies.\n",
        "\n",
        "* `tensor2tensor`: a library with all necessary tools to perform training/inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPGni6fuvoTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Tensor2tensor\n",
        "!pip install -q -U tensor2tensor\n",
        "\n",
        "print('All done.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udP-H4wlNdX4",
        "colab_type": "text"
      },
      "source": [
        "# Setup some options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uza9j3ISNhls",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Imports we need.\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import collections\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "#@markdown 1. The problem is either `translate_vien_iwslt32k` or `translate_envi_iwslt32k`\n",
        "\n",
        "problem = 'translate_envi_iwslt32k'  # @param\n",
        "\n",
        "#@markdown 2. We use the tiny setting of the transformer by default.\n",
        "\n",
        "hparams_set = 'transformer_tiny'  # @param\n",
        "\n",
        "#@markdown 3. Next we specify the directory where all data involving this colab will be stored (training data, checkpoints, decoded text etc.)\n",
        "\n",
        "#@markdown * For GPU we use Google Drive Storage (free for everyone with a Google account, no need to install any payment method).\n",
        "\n",
        "google_drive_dir = 'back_translate'  # @param\n",
        "\n",
        "\n",
        "#@markdown * With TPU, unfortunately only Google Cloud Storage is usable (free trial with a payment method required). Here we specify a Storage bucket.\n",
        "\n",
        "google_cloud_bucket = 'vien-translation'  # @param\n",
        "\n",
        "#@markdown Please note that only one of the two options above will be used depending on which runtime setting you are using.\n",
        "\n",
        "#@markdown 4. Now we specify all sub-directories:\n",
        "\n",
        "#@markdown * Data tfrecords (train/valid) to train/eval on will be generated to:\n",
        "data_dir = 'data/translate_envi_iwslt32k'  # @param\n",
        "\n",
        "#@markdown * Save/load checkpoints to here:\n",
        "logdir = 'checkpoints/translate_envi_iwslt32k_tiny'  # @param\n",
        "\n",
        "#@markdown * The temporary dir to store all the temp files during data generation (e.g. downloads from the internet).\n",
        "\n",
        "tmp_dir = 'raw'  # @param\n",
        "\n",
        "is_demo = True  # @param {type: \"boolean\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUSADz_ti63",
        "colab_type": "text"
      },
      "source": [
        "# Create and mount to Cloud/Drive Storage, connect to GPU or TPU runtime.\n",
        "\n",
        "Now we create all the directories and mount them to the colab so that python packages here (e.g. `os.path.exists`) can see and work on them. \n",
        "\n",
        "* For TPUs, we will have access to a cluster of 2x2 chips (i.e. 8 cores because each chip has 2 cores). One complete replication of the TF graph will be placed on each core, data parallelism is done through this.\n",
        "\n",
        "* The address of the TPUs on the cloud will also be needed to pass to tensor2tensor while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oILRLCWN_16u",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Check if the runtime is set to TPU or GPU:\n",
        "use_tpu = 'COLAB_TPU_ADDR' in os.environ\n",
        "\n",
        "\n",
        "def setup_gpu():\n",
        "  # Mount \"My Drive\" into /content/drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  tpu_address = ''\n",
        "  mount_point = '/content/drive/My Drive/{}'.format(google_drive_dir)\n",
        "  return mount_point\n",
        "  \n",
        "  \n",
        "def setup_tpu():\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "  # First we Connect to the TPU pod.\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "  # Mount the bucket to colab, so that python package os can access to it.\n",
        "  # First we install gcsfuse to be able to mount Google Cloud Storage with Colab.\n",
        "  print('\\nInstalling gcsfuse')\n",
        "  !echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "  !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "  !apt -qq update\n",
        "  !apt -qq install gcsfuse\n",
        "\n",
        "  bucket = google_cloud_bucket\n",
        "  print('Mounting bucket {} to local.'.format(bucket))\n",
        "  mount_point = '/content/{}'.format(bucket)\n",
        "  if not os.path.exists(mount_point):\n",
        "    tf.gfile.MakeDirs(mount_point)\n",
        "  \n",
        "  !fusermount -u $mount_point\n",
        "  !gcsfuse --implicit-dirs $bucket $mount_point\n",
        "  print('\\nMount point content:')\n",
        "  !ls $mount_point\n",
        "\n",
        "  return mount_point, tpu_address\n",
        "\n",
        "\n",
        "if not use_tpu:\n",
        "  mount_point = setup_gpu()\n",
        "  tpu_address = ''\n",
        "else:\n",
        "  mount_point, tpu_address = setup_tpu()\n",
        "  \n",
        "print('\\nMount point: {}'.format(mount_point))\n",
        "print('TPU address: {}'.format(tpu_address))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHipozkT-VCZ",
        "colab_type": "text"
      },
      "source": [
        "Now we create all the directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3dqhO8v-Uj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we make all the paths absolute.\n",
        "logdir = os.path.join(mount_point, logdir)\n",
        "data_dir = os.path.join(mount_point, data_dir)\n",
        "tmp_dir = os.path.join(mount_point, tmp_dir)\n",
        "tf.gfile.MakeDirs(logdir)\n",
        "tf.gfile.MakeDirs(data_dir)\n",
        "tf.gfile.MakeDirs(tmp_dir)\n",
        "\n",
        "if is_demo:\n",
        "  run_logdir = os.path.join(logdir, 'demo')\n",
        "  if tf.gfile.Exists(run_logdir):\n",
        "    tf.gfile.DeleteRecursively(run_logdir)\n",
        "else:\n",
        "  run_logdir = logdir\n",
        "\n",
        "print('log dir: {}'.format(run_logdir))\n",
        "print('data dir: {}'.format(data_dir))\n",
        "print('temp dir: {}'.format(tmp_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaDPi48oH3ug",
        "colab_type": "text"
      },
      "source": [
        "# Clone or Pull source code from our Github repo `vietai/dab`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsttrcZbH2t0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = '/content/dab'\n",
        "if not os.path.exists(src):\n",
        "  ! git clone https://github.com/vietai/dab.git\n",
        "else:\n",
        "  % cd $src\n",
        "  ! git pull\n",
        "  % cd /\n",
        "\n",
        "print('\\n Source code:')\n",
        "!ls $src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHjPnQzpR9PO",
        "colab_type": "text"
      },
      "source": [
        "# Generate Training and Validation datasets\n",
        "\n",
        "First let's look at what the training data looks like in its original text format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_JAag9Zkcaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 10 $tmp_dir/train.en\n",
        "print('=' * 10)\n",
        "!head -n 10 $tmp_dir/train.vi\n",
        "\n",
        "# Now we count the number of lines:\n",
        "print('\\nNumber of training text lines:')\n",
        "!wc -l $tmp_dir/train.en\n",
        "!wc -l $tmp_dir/train.vi\n",
        "print('\\nNumber of validation text lines:')\n",
        "!wc -l $tmp_dir/tst2012.en\n",
        "!wc -l $tmp_dir/tst2012.vi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzckUIG4kdzi",
        "colab_type": "text"
      },
      "source": [
        "`tensor2tensor` requests the data to be in a certain format to be efficiently handled in its training/inference pipeline (e.g. parallel access, shuffling, distributed storage, etc). Use the following command to:\n",
        "\n",
        "* Download raw text training/dev data from the internet into `tmp_dir`.\n",
        "* Proprocess and tokenize the raw text to build a vocabulary.\n",
        "* Turn the original text format of the data into the expected format `tfrecords` and store them in `data_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrtLxSzR8lx",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "!python $src/t2t_datagen.py --data_dir=$data_dir \\\n",
        "--tmp_dir=$tmp_dir --problem=$problem\n",
        "\n",
        "print('\\nGenerated TF records:')\n",
        "!ls $data_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JILbdybjBdFm",
        "colab_type": "text"
      },
      "source": [
        "Let's also look at the vocabulary file. Each token will be on a line and they are of the decreasing frequency order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULOhirr0Bdn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = os.path.join(data_dir, 'vocab.{}.32768.subwords'.format(problem))\n",
        "!head -n 10 $vocab\n",
        "!tail -n 10 $vocab\n",
        "!wc -l $vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a69r1KDiZDe",
        "colab_type": "text"
      },
      "source": [
        "# Run Training\n",
        "\n",
        "Instead of using the default option (250K training steps), the model only need ~50K steps to converge (and overfit without regularization).\n",
        "\n",
        "* On GPU, this will take ~ half a day. Evaluation on the validation set will be done intermittenly in-between training for every 1000 steps.\n",
        "\n",
        "* On TPU, this will take ~ half an hour. Evaluation on the validation set will be done once training is finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYDMO4zArgkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_steps = 1000 if is_demo else 50000\n",
        "\n",
        "if use_tpu:\n",
        "  # TPU wants the address to begin with gs://\n",
        "  train_output_dir = run_logdir.replace(mount_point, 'gs://{}'.format(google_cloud_bucket))\n",
        "  train_data_dir = data_dir.replace(mount_point, 'gs://{}'.format(google_cloud_bucket))\n",
        "\n",
        "!python $src/t2t_trainer.py --model='transformer' --hparams_set=$hparams_set \\\n",
        "--hparams='learning_rate_cosine_cycle_steps=50000' \\\n",
        "--train_steps=$train_steps --eval_steps=10 \\\n",
        "--problem=$problem --data_dir=$train_data_dir \\\n",
        "--output_dir=$train_output_dir --use_tpu=$use_tpu --cloud_tpu_name=$tpu_address"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu54Ypz54bep",
        "colab_type": "text"
      },
      "source": [
        "# Launch Tensorboard\n",
        "\n",
        "We launch tensorboard before training, the tensorboard will update its content in real time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyUkIIYG4df0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "print('Reading events from {}'.format(run_logdir))\n",
        "%tensorboard --logdir=$run_logdir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-wU9BdLd4hQ",
        "colab_type": "text"
      },
      "source": [
        "# Download test data\n",
        "\n",
        "So far we have trained and evaluated the model on Train/Dev sets. Now we download the Test set and perform decoding on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d6X7Np0d4Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd $tmp_dir\n",
        "!wget \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\"\n",
        "!tar -xzf test-2013-en-vi.tgz\n",
        "%cd /\n",
        "\n",
        "print('\\nSample test data:')\n",
        "!head -n 10 $tmp_dir/tst2013.en\n",
        "print('=' * 10)\n",
        "!head -n 10 $tmp_dir/tst2013.vi\n",
        "\n",
        "print('\\nTest data size:')\n",
        "!wc -l $tmp_dir/tst2013.en\n",
        "!wc -l $tmp_dir/tst2013.vi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFtIdFfgzRJI",
        "colab_type": "text"
      },
      "source": [
        "# Compute Test set BLEU score using the final checkpoint\n",
        "\n",
        "The BLEU (BiLingual Evaluation Understudy) score measures n-grams overlapping between the translated text and reference text (i.e. ground truth). It is shown to be correlated well with human judgement. Although there has been some criticism, BLEU score has been one of the most widely used auto metric to evaluate any translation model in Machine Learning.\n",
        "\n",
        "There are two steps to compute BLEU score:\n",
        "\n",
        "1. Translate the source text file `tst2013.vi` to English.\n",
        "2. Compare the output of Step 1 with the reference `tst2013.en`.\n",
        "\n",
        "Use the following command for Step 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIAp4ctWufFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode_from_file = os.path.join(tmp_dir, 'tst2013.en')\n",
        "decode_to_file = os.path.join(tmp_dir, 'tiny.tst2013.en2vi.txt')\n",
        "ref_file = os.path.join(tmp_dir, 'tst2013.vi')\n",
        "  \n",
        "if use_tpu:\n",
        "  # TPU wants the paths to begin with gs://\n",
        "  ckpt_dir = logdir.replace(mount_point, 'gs://{}'.format(google_cloud_bucket))\n",
        "\n",
        "print('Decode to file {}'.format(decode_to_file))\n",
        "!python $src/t2t_decoder.py \\\n",
        "--data_dir=$train_data_dir --problem=$problem \\\n",
        "--hparams_set=$hparams_set \\\n",
        "--model=transformer \\\n",
        "--decode_hparams=\"beam_size=4,alpha=0.6\"  \\\n",
        "--decode_from_file=$decode_from_file \\\n",
        "--decode_to_file=$decode_to_file  \\\n",
        "--output_dir=$ckpt_dir \\\n",
        "--use_tpu=$use_tpu \\\n",
        "--cloud_tpu_name=$tpu_address"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W__dAgiKO50",
        "colab_type": "text"
      },
      "source": [
        "Now let's look at the translated text and compare it to the reference text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbRMoVNSKNas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wc -l $decode_to_file\n",
        "!wc -l $ref_file\n",
        "\n",
        "!head -n 5 $decode_to_file\n",
        "!tail -n 5 $decode_to_file\n",
        "print('=' * 10)\n",
        "!head -n 5 $ref_file\n",
        "!tail -n 5 $ref_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHWLuvKwKmgh",
        "colab_type": "text"
      },
      "source": [
        "Now use the following command to compute the BLEU score (Step 2):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrgbhS9aKnyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\nCompare {} with reference {}'.format(decode_to_file, ref_file))\n",
        "!t2t-bleu --translation=$decode_to_file --reference=$ref_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPSEQny90P3z",
        "colab_type": "text"
      },
      "source": [
        "# Compute BLEU score by averaging the latest 20 checkpoints.\n",
        "\n",
        "A very effective and powerful technique to improve test performance of neural networks is to average a few last checkpoints. Let's try doing so and see if there is any improvement. First we need to use `t2t-avg-all` from the `tensor2tensor` library to average all the checkpoints available - which is 20 - the number of checkpoints that got kept during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KorWEy_af_1i",
        "colab": {}
      },
      "source": [
        "decode_to_file = os.path.join(tmp_dir, 'tiny.avg.tst2013.en2vi.txt')\n",
        " \n",
        "if use_tpu:\n",
        "  # TPU wants the paths to begin with gs://\n",
        "  ckpt_dir = logdir.replace(mount_point, 'gs://{}'.format(google_cloud_bucket))\n",
        "\n",
        "avg_dir = os.path.join(ckpt_dir, 'avg')\n",
        "avg_ckpt = os.path.join(avg_dir, 'model.ckpt-50000.index')\n",
        "\n",
        "print('Averaging..')\n",
        "if not tf.gfile.Exists(avg_ckpt):\n",
        "  !t2t-avg-all --model_dir=$logdir --output_dir=$avg_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbZ1hGfhLcZJ",
        "colab_type": "text"
      },
      "source": [
        "Now we repeat the same step as above to compute the BLEU score, this time reviving from the `avg_dir` to have the averaged checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni7TGM5LLcDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Decoding..')\n",
        "!python $src/t2t_decoder.py --data_dir=$data_dir \\\n",
        "--problem=$problem \\\n",
        "--hparams_set=$hparams_set \\\n",
        "--model=transformer --decode_hparams=\"beam_size=4,alpha=0.6\"  \\\n",
        "--decode_from_file=$decode_from_file \\\n",
        "--decode_to_file=$decode_to_file  \\\n",
        "--output_dir=$avg_dir \\\n",
        "--use_tpu=$use_tpu \\\n",
        "--cloud_tpu_name=$tpu_address\n",
        "\n",
        "print('Compute BLEU score..')\n",
        "!t2t-bleu --translation=$decode_to_file \\\n",
        "--reference=$ref_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr-Tbz8JMBss",
        "colab_type": "text"
      },
      "source": [
        "If you did everything right, there should be at least a +1.0 improvement in BLEU score for the settings in this Colab! And that's the end of this tutorial, happy training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_hJNyHqMOaI",
        "colab_type": "text"
      },
      "source": [
        "# Acknowledgements\n",
        "\n",
        "This work is made possible by [VietAI](http://vietai.org/). Special thanks to [Thang Luong](http://thangluong.com), Le Cao Thang, and Hoang Quy Phat for collaborating and giving comments.\n",
        "\n",
        "# References\n",
        "\n",
        "1. Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems. 2017.\n",
        "\n",
        "2. Izmailov, Pavel, et al. \"Averaging weights leads to wider optima and better generalization.\" arXiv preprint arXiv:1803.05407 (2018).\n",
        "\n",
        "3. Vaswani, Ashish, et al. \"Tensor2tensor for neural machine translation.\" arXiv preprint arXiv."
      ]
    }
  ]
}
